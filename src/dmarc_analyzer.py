import os
import sys
import xml.etree.ElementTree as ET
import gzip
import zipfile
from datetime import datetime
from elasticsearch import Elasticsearch, ConnectionError 
import time
import shutil
from pathlib import Path
import logging
from es_setup import setup_elasticsearch_indices
from analysis_utils import (
    calculate_authentication_rates,
    generate_report_stats,
    check_duplicate_report,
    batch_save_documents
)




# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DMARCAnalyzer:
    def __init__(self, report_directory, extract_directory, es_url, domain: str):
        """
        DMARCã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        Args:
            report_directory: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            extract_directory: å±•é–‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            es_url: Elasticsearchã®URL
            domain: è§£æå¯¾è±¡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³
        """
        self.report_directory = report_directory
        self.extract_directory = extract_directory
        self.es_url = es_url
        self.processed_files = set()
        
        # åˆæœŸåŒ–ã‚’__init__ã§å®Ÿè¡Œ
        self._initialize()
        
    

    def _initialize(self):
        """åˆæœŸåŒ–å‡¦ç†"""
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            Path(self.extract_directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"Extract directory created/confirmed: {self.extract_directory}")
            
            # Elasticsearchã¸ã®æ¥ç¶š
            self._connect_elasticsearch()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®š
            setup_elasticsearch_indices(self.es)
            logger.info("Elasticsearch indices setup completed")
            
        except Exception as e:
            logger.error(f"Initialization error: {e}")
            raise

    def _connect_elasticsearch(self):
        """Elasticsearchã¸ã®æ¥ç¶šã‚’ç¢ºç«‹"""
        retries = 5
        for i in range(retries):
            try:
                self.es = Elasticsearch([self.es_url])
                if self.es.ping():
                    logger.info("Successfully connected to Elasticsearch")
                    break
            except ConnectionError:
                logger.warning(f"Connection attempt {i+1} failed, retrying...")
                time.sleep(10)
            except Exception as e:
                logger.error(f"Unexpected error: {e}")
                time.sleep(10)

        if not hasattr(self, 'es') or not self.es.ping():
            raise ConnectionError("Failed to connect to Elasticsearch")

    def analyze_reports(self):
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        logger.info(f"Analyzing reports in directory: {self.report_directory}")
        
        if not os.path.exists(self.report_directory):
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.report_directory}")
            return
            
        if not os.path.isdir(self.report_directory):
            logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {self.report_directory}")
            return
            
        files = os.listdir(self.report_directory)
        logger.info(f"Found {len(files)} files in directory")
        for file in files:
            logger.info(f"Found file: {file}")
        
        try:
            for root, _, files in os.walk(self.report_directory):
                for file in files:
                    file_path = os.path.join(root, file)
                    if file.endswith(".xml"):
                        logger.info(f"Processing XML file: {file_path}")
                        self.process_report_file(file_path)
                    elif file.endswith((".zip", ".gz")):
                        logger.info(f"Extracting archive file: {file_path}")
                        extracted_files = self._extract_nested_archives(file_path, self.extract_directory)
                        for extracted_file in extracted_files:
                            if extracted_file.endswith(".xml"):
                                logger.info(f"Processing extracted XML file: {extracted_file}")
                                self.process_report_file(extracted_file)
        except Exception as e:
            logger.error(f"Error analyzing reports: {e}")
            raise

    def process_report_file(self, file_path):
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆãƒ­ãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰"""
        lock_file = file_path + '.lock'
        if os.path.exists(lock_file):
            logger.info(f"File is being processed: {file_path}")
            return

        try:
            with open(lock_file, 'w') as f:
                f.write(str(datetime.now()))
            
            self.process_xml_file(file_path)
            
        finally:
            if os.path.exists(lock_file):
                os.remove(lock_file)

    def process_xml_file(self, xml_file_path: str) -> None:
        """XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å‡¦ç†ã—ã¦Elasticsearchã«ä¿å­˜"""
        logger.info(f"XMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†é–‹å§‹: {xml_file_path}")
        
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å­˜åœ¨ç¢ºèªã¨ä½œæˆ
            current_month = datetime.now().strftime("%Y.%m")
            aggregate_index = f"aggregate_reports-{current_month}"
            forensic_index = f"forensic_reports-{current_month}"

            if not (self.es.indices.exists(index=aggregate_index) and 
                   self.es.indices.exists(index=forensic_index)):
                setup_elasticsearch_indices(self.es)
                logger.info("Created required indices")

            # XMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹
            tree = ET.parse(xml_file_path)
            root = tree.getroot()
            
            # ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            is_forensic = False
            
            # ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ç‰¹å¾´ã‚’ç¢ºèª
            if root.tag == 'feedback':
                feedback_type = root.find('feedback-type')
                if feedback_type is not None and feedback_type.text == 'failure report':
                    is_forensic = True
                elif 'forensic' in xml_file_path.lower():
                    is_forensic = True
            
            if is_forensic:
                # ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®å‡¦ç†
                report_data = self._parse_forensic_report(root)
                if report_data:
                    self._save_forensic_report(report_data)
                    logger.info(f"ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {xml_file_path}")
                else:
                    logger.warning(f"ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {xml_file_path}")
            else:
                # é›†è¨ˆãƒ¬ãƒãƒ¼ãƒˆã®å‡¦ç†
                report_data = self._parse_aggregate_report(root)
                if report_data:
                    self._save_aggregate_report(report_data)
                    logger.info(f"é›†è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {xml_file_path}")
                else:
                    logger.warning(f"é›†è¨ˆãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {xml_file_path}")
                    
        except ET.ParseError as e:
            logger.error(f"XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ {xml_file_path}: {e}")
        except Exception as e:
            logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ {xml_file_path}: {e}", exc_info=True)

    def _parse_aggregate_report(self, root):
        """é›†è¨ˆãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            report_metadata = root.find('report_metadata')
            if report_metadata is None:
                logger.warning("No report_metadata found in XML")
                return None

            policy_published = root.find('policy_published')
            if policy_published is None:
                logger.warning("No policy_published found in XML")
                return None

            parsed_data = {
                "org_name": report_metadata.findtext('org_name', 'unknown'),
                "report_id": report_metadata.findtext('report_id', 'unknown'),
                "report_date": None,
                "date_range": {
                    "begin": None,
                    "end": None
                },
                "policy_published": {
                    "domain": policy_published.findtext('domain', 'unknown'),
                    "adkim": policy_published.findtext('adkim', 'unknown'),
                    "aspf": policy_published.findtext('aspf', 'unknown'),
                    "p": policy_published.findtext('p', 'unknown'),
                    "sp": policy_published.findtext('sp', 'unknown'),
                    "pct": policy_published.findtext('pct', 'unknown')
                },
                "records": []
            }

            date_range = report_metadata.find('date_range')
            if date_range is not None:
                begin = date_range.findtext('begin')
                end = date_range.findtext('end')
                if begin:
                    begin_date = datetime.fromtimestamp(int(begin))
                    parsed_data['date_range']['begin'] = begin_date.isoformat()
                    parsed_data['report_date'] = begin_date.date().isoformat()
                if end:
                    parsed_data['date_range']['end'] = datetime.fromtimestamp(int(end)).isoformat()

            for record in root.findall('record'):
                row = record.find('row')
                identifiers = record.find('identifiers')
                auth_results = record.find('auth_results')
                
                if row is not None:
                    record_data = {
                        "source_ip": row.findtext('source_ip', 'unknown'),
                        "count": int(row.findtext('count', '0')),
                        "policy_evaluated": {},
                        "auth_results": {},
                        "header_from": "unknown"
                    }

                    policy_evaluated = row.find('policy_evaluated')
                    if policy_evaluated is not None:
                        record_data["policy_evaluated"] = {
                            "disposition": policy_evaluated.findtext('disposition', 'none'),
                            "dkim": policy_evaluated.findtext('dkim', 'none'),
                            "spf": policy_evaluated.findtext('spf', 'none')
                        }

                    if auth_results is not None:
                        record_data["auth_results"] = {
                            "dkim": auth_results.findtext('.//dkim/result', 'none'),
                            "spf": auth_results.findtext('.//spf/result', 'none')
                        }

                    if identifiers is not None:
                        record_data["header_from"] = identifiers.findtext('header_from', 'unknown')

                    parsed_data["records"].append(record_data)

            logger.debug(f"Parsed aggregate report data: {parsed_data}")
            return parsed_data

        except Exception as e:
            logger.error(f"Error parsing aggregate report: {e}")
            return None

    def _parse_forensic_report(self, root):
        """ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            parsed_data = {
                "@timestamp": datetime.now().isoformat(),
                "report_date": datetime.now().isoformat(),
                "feedback_type": "failure report",
                "version": root.findtext('version', '1.0'),
                "report_metadata": {
                    "org_name": root.findtext('.//org_name', 'unknown'),
                    "email": root.findtext('.//email', 'unknown'),
                    "report_id": root.findtext('.//report_id', f"FR-{int(datetime.now().timestamp())}"),
                    "date_range": datetime.now().isoformat()
                }
            }

            # Identity Alignment
            identity_alignment = root.find('identity_alignment')
            if identity_alignment is not None:
                parsed_data["identity_alignment"] = {
                    "dkim": identity_alignment.findtext('dkim', 'false').lower() == 'true',
                    "spf": identity_alignment.findtext('spf', 'false').lower() == 'true'
                }

            # Original Mail Data ã¨IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®æŠ½å‡º
            original_mail = root.find('original_mail_data')
            if original_mail is not None and original_mail.text:
                import base64
                try:
                    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿æŒ
                    original_encoding = original_mail.get('encoding', 'base64')
                    
                    # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
                    decoded_content = base64.b64decode(original_mail.text).decode('utf-8', errors='ignore')
                    
                    # ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜
                    parsed_data["original_mail_data"] = {
                        "content": decoded_content,
                        "encoding": original_encoding
                    }
                    
                except Exception as e:
                    logger.warning(f"Failed to decode mail content: {e}")
                    parsed_data["original_mail_data"] = {
                        "content": original_mail.text,
                        "encoding": original_mail.get('encoding', 'unknown')
                    }

            # Source Information
            source = root.find('.//source')
            if source is not None:
                ip_address = source.findtext('ip_address', '0.0.0.0')
                smtp_hostname = source.findtext('smtp_hostname', 'unknown')
                parsed_data["source"] = {
                    "ip_address": ip_address,
                    "smtp_hostname": smtp_hostname
                }
            else:
                parsed_data["source"] = {
                    "ip_address": "0.0.0.0",
                    "smtp_hostname": "unknown"
                }
                logger.warning("No source information found in report")

            # Auth Results with human readable results
            auth_results = root.find('auth_results')
            if auth_results is not None:
                parsed_data["auth_results"] = {
                    "dkim": {
                        "domain": auth_results.findtext('.//dkim/domain', 'unknown'),
                        "selector": auth_results.findtext('.//dkim/selector', 'unknown'),
                        "result": auth_results.findtext('.//dkim/result', 'none'),
                        "human_result": self._get_human_readable_result('dkim', auth_results.findtext('.//dkim/result', 'none'))
                    },
                    "spf": {
                        "domain": auth_results.findtext('.//spf/domain', 'unknown'),
                        "scope": auth_results.findtext('.//spf/scope', 'unknown'),
                        "result": auth_results.findtext('.//spf/result', 'none'),
                        "human_result": self._get_human_readable_result('spf', auth_results.findtext('.//spf/result', 'none'))
                    },
                    "dmarc": {
                        "domain": auth_results.findtext('.//dmarc/domain', 'unknown'),
                        "result": auth_results.findtext('.//dmarc/result', 'none'),
                        "human_result": self._get_human_readable_result('dmarc', auth_results.findtext('.//dmarc/result', 'none'))
                    }
                }

            # Failure Details
            failure_details = root.find('failure_details')
            if failure_details is not None:
                parsed_data["failure_details"] = {
                    "reason": failure_details.findtext('reason', 'unknown')
                }

            return parsed_data

        except Exception as e:
            logger.error(f"Error parsing forensic report: {e}", exc_info=True)
            return None

    def _get_human_readable_result(self, auth_type: str, result: str) -> str:
        """èªè¨¼çµæœã®ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒªãƒ¼ãƒ€ãƒ–ãƒ«ãªèª¬æ˜ã‚’ç”Ÿæˆ"""
        explanations = {
            'dkim': {
                'pass': 'é›»å­ç½²åãŒæ­£å¸¸ã«æ¤œè¨¼ã•ã‚Œã¾ã—ãŸ',
                'fail': 'é›»å­ç½²åã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'neutral': 'æ¤œè¨¼çµæœã¯ä¸­ç«‹ã§ã™',
                'none': 'é›»å­ç½²åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
            },
            'spf': {
                'pass': 'é€ä¿¡å…ƒIPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯æ‰¿èªã•ã‚Œã¦ã„ã¾ã™',
                'fail': 'é€ä¿¡å…ƒIPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯æ‰¿èªã•ã‚Œã¦ã„ã¾ã›ã‚“',
                'softfail': 'é€ä¿¡å…ƒIPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯ç–‘ã‚ã—ã„ã¨ãƒãƒ¼ã‚¯ã•ã‚Œã¦ã„ã¾ã™',
                'neutral': 'æ¤œè¨¼çµæœã¯ä¸­ç«‹ã§ã™',
                'none': 'SPFãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
            },
            'dmarc': {
                'pass': 'DMARCãƒãƒªã‚·ãƒ¼ã«æº–æ‹ ã—ã¦ã„ã¾ã™',
                'fail': 'DMARCãƒãƒªã‚·ãƒ¼ã«é•åã—ã¦ã„ã¾ã™',
                'none': 'DMARCãƒãƒªã‚·ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
            }
        }
        
        return explanations.get(auth_type, {}).get(result.lower(), 'ä¸æ˜ãªçµæœã§ã™')

    def _parse_mail_headers(self, mail_content: str) -> dict:
        """ãƒ¡ãƒ¼ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‘ãƒ¼ã‚¹"""
        if not mail_content:
            return {}

        headers = {
            'from': '',
            'to': '',
            'subject': '',
            'date': None
        }

        try:
            # ãƒ¡ãƒ¼ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã‚’æŠ½å‡º
            header_lines = []
            for line in mail_content.split('\n'):
                if line.strip() == '':
                    break
                header_lines.append(line)

            current_header = None
            current_value = ''

            for line in header_lines:
                if line.startswith(' ') or line.startswith('\t'):
                    # ç¶™ç¶šè¡Œ
                    current_value += ' ' + line.strip()
                else:
                    # æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼
                    if current_header:
                        if current_header.lower() in headers:
                            headers[current_header.lower()] = current_value.strip()
                    
                    if ':' in line:
                        current_header, value = line.split(':', 1)
                        current_header = current_header.lower()
                        current_value = value.strip()

            # æœ€å¾Œã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å‡¦ç†
            if current_header and current_header.lower() in headers:
                headers[current_header.lower()] = current_value.strip()

            # æ—¥ä»˜ã®å¤‰æ›
            if headers['date']:
                try:
                    headers['date'] = datetime.strptime(
                        headers['date'], 
                        '%a, %d %b %Y %H:%M:%S %z'
                    ).isoformat()
                except ValueError:
                    headers['date'] = None

        except Exception as e:
            logger.error(f"Error parsing mail headers: {e}")

        return headers
    
    def _is_valid_ip(self, ip_string: str) -> bool:
        """IPã‚¢ãƒ‰ãƒ¬ã‚¹ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        if not ip_string or ip_string.lower() == 'unknown':
            return False
            
        import re
        # ç°¡æ˜“çš„ãªIPv4ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern = r'^(\d{1,3}\.){3}\d{1,3}$'
        if not re.match(pattern, ip_string):
            return False
        
        # å„ã‚ªã‚¯ãƒ†ãƒƒãƒˆãŒ0-255ã®ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
        try:
            return all(0 <= int(octet) <= 255 for octet in ip_string.split('.'))
        except (ValueError, AttributeError):
            return False
        

    def _save_aggregate_report(self, report):
        """é›†è¨ˆãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        try:
            if 'records' not in report:
                logger.warning("No records found in aggregate report")
                return

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if check_duplicate_report(self.es, report['report_id']):
                logger.info(f"Skipping duplicate report: {report['report_id']}")
                return

            # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            base_metadata = {k: v for k, v in report.items() if k != 'records'}
            base_metadata.update({
                'analyzed_at': datetime.now().isoformat()
            })

            # èªè¨¼ç‡ã®è¨ˆç®—
            auth_rates = calculate_authentication_rates(report['records'])
            base_metadata.update(auth_rates)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã«æ—¥ä»˜ã‚’å«ã‚ã‚‹
            index_name = f"aggregate_reports-{datetime.now():%Y.%m}"

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
            documents = []
            for record in report['records']:
                document = {**base_metadata, **record}
                documents.append(document)

            # ãƒãƒƒãƒä¿å­˜
            batch_save_documents(self.es, documents, index_name)

            # çµ±è¨ˆæƒ…å ±ã®ç”Ÿæˆã¨ä¿å­˜
            stats = generate_report_stats(documents, report['policy_published']['domain'])
            self.es.index(
                index=f"dmarc_stats-{datetime.now():%Y.%m}",
                document={
                    'stats': stats,
                    'timestamp': datetime.now().isoformat()
                }
            )

        except Exception as e:
            logger.error(f"Error saving aggregate report: {e}")

    def _save_forensic_report(self, report):
        """ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        try:
            current_month = datetime.now().strftime("%Y.%m")
            index_name = f"forensic_reports-{current_month}"
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¿½åŠ 
            current_time = datetime.now()
            report.update({
                '@timestamp': current_time.isoformat(),
                'report_date': current_time.isoformat()
            })
            
            response = self.es.index(
                index=index_name,
                document=report
            )
            logger.info(f"ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {response['result']}")
        except Exception as e:
            logger.error(f"ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)

    def _extract_nested_archives(self, file_path, extract_dir):
        """å†å¸°çš„ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å±•é–‹"""
        logger.debug(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®å±•é–‹é–‹å§‹: {file_path}")
        
        try:
            temp_dir = os.path.join(
                extract_dir, 
                f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
            )
            os.makedirs(temp_dir, exist_ok=True)
            logger.debug(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {temp_dir}")

            extracted_files = []

            try:
                if zipfile.is_zipfile(file_path):
                    logger.debug(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†: {file_path}")
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        for file_info in zip_ref.filelist:
                            try:
                                file_name = file_info.filename
                                extracted_path = os.path.join(temp_dir, os.path.basename(file_name))
                                
                                with zip_ref.open(file_info) as source, \
                                    open(extracted_path, 'wb') as target:
                                    shutil.copyfileobj(source, target)

                                if file_name.lower().endswith(('.zip', '.gz')):
                                    nested_files = self._extract_nested_archives(extracted_path, extract_dir)
                                    extracted_files.extend(nested_files)
                                    os.remove(extracted_path)
                                elif file_name.lower().endswith('.xml'):
                                    final_path = os.path.join(extract_dir, 
                                        f"{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{os.path.basename(file_name)}")
                                    shutil.move(extracted_path, final_path)
                                    extracted_files.append(final_path)
                            except Exception as e:
                                logger.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_name}: {e}")

                elif file_path.endswith('.gz'):
                    logger.debug(f"GZãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†: {file_path}")
                    output_path = os.path.join(temp_dir, 
                        f"{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{os.path.basename(file_path)[:-3]}")
                    
                    with gzip.open(file_path, 'rb') as f_in:
                        with open(output_path, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)

                    if output_path.lower().endswith('.zip'):
                        nested_files = self._extract_nested_archives(output_path, extract_dir)
                        extracted_files.extend(nested_files)
                        os.remove(output_path)
                    elif output_path.lower().endswith('.xml'):
                        final_path = os.path.join(extract_dir, os.path.basename(output_path))
                        shutil.move(output_path, final_path)
                        extracted_files.append(final_path)

                return extracted_files

            finally:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    logger.debug(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤: {temp_dir}")

        except Exception as e:
            logger.error(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±•é–‹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return []

    

if __name__ == "__main__":
    print("\n=== DMARCè§£æã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ã—ã¾ã™ ===")
    logger.info("Starting DMARC Analyzer")
    
    try:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        default_domain = os.getenv("DEFAULT_DOMAIN", "admin")
        logger.info(f"Using default domain: {default_domain}")
        print(f"è§£æå¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {default_domain}")
            
        # DMARCAnalyzerã®åˆæœŸåŒ–
        analyzer = DMARCAnalyzer(
            report_directory="/app/files", 
            extract_directory="/app/files/extracted",
            es_url="http://elasticsearch:9200",
            domain=default_domain
        )
        
        # åˆæœŸã®è§£æã‚’å®Ÿè¡Œ
        print("\nğŸ” åˆæœŸè§£æã‚’é–‹å§‹ã—ã¾ã™...")
        analyzer.analyze_reports()
        print("âœ… åˆæœŸè§£æãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info("Initial DMARC analysis completed")
        
        # Grafana URLã®è¡¨ç¤º
        print("\nğŸ“Š Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™:")
        print("URL: http://localhost:3000")
        print("ãƒ¦ãƒ¼ã‚¶ãƒ¼å: admin")
        print("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰: admin")
        
        print("\n=== ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™ ===")
        print(f"ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: /app/files")
        logger.info("Starting file monitoring")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³
        if not os.path.exists("/app/files"):
            print("âŒ ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            logger.error("Monitor directory does not exist")
            os.makedirs("/app/files")
            print("âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
            
        permissions = oct(os.stat("/app/files").st_mode)[-3:]
        print(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³: {permissions}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’è¡¨ç¤º
        print("\nğŸ“‚ ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹:")
        for root, dirs, files in os.walk("/app/files"):
            print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {root}")
            for d in dirs:
                print(f" - Dir: {d}")
            for f in files:
                print(f" - File: {f}")
        
            
    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)